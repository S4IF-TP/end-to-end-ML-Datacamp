{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee58dba",
   "metadata": {},
   "source": [
    "# Module 6: Testing and Deployment\n",
    "\n",
    "**Course**: End-to-End Machine Learning (Datacamp)  \n",
    "**Case Study**: CardioCare Heart Disease Prediction  \n",
    "**Author**: Seif\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this module, you'll:\n",
    "- Understand why testing is critical pre-deployment\n",
    "- Learn Python's built-in `unittest` framework\n",
    "- Write tests for model inference (shape, speed, value ranges)\n",
    "- Run tests automatically within the notebook\n",
    "\n",
    "Testing ensures our model doesn't crash and returns predictions quickly and in correct formatsâ€”especially important when cardiologists rely on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98412194",
   "metadata": {},
   "source": [
    "## Why testing before deployment?\n",
    "\n",
    "- Catch failures before clinicians see them\n",
    "- Verify latency (inference time) is acceptable\n",
    "- Check predictions have the right type and shape\n",
    "- Validate inputs fall within expected ranges\n",
    "\n",
    "We'll use `unittest.TestCase` to define focused tests and run them automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac76698",
   "metadata": {},
   "source": [
    "## Unittest basics\n",
    "\n",
    "- Create a test class by subclassing `unittest.TestCase`\n",
    "- Name test methods starting with `test_...`\n",
    "- Use assertions like `assertEqual`, `assertTrue`, `assertIn`, etc.\n",
    "- Run with `unittest.main()` (we'll use a notebook-friendly variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033332e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a small model and data for testing purposes\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Use synthetic data as a stand-in for CardioCare inputs\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=8, n_informative=5, n_redundant=2,\n",
    "    random_state=42, class_sep=1.2\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "deployed_model = LogisticRegression(max_iter=1000, solver=\"liblinear\", random_state=42)\n",
    "deployed_model.fit(X_train, y_train)\n",
    "\n",
    "# Helper: simple inference function (could be an API endpoint in prod)\n",
    "def predict_labels(model, X):\n",
    "    return model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2406f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write unit tests for inference behavior\n",
    "import unittest, time\n",
    "import numpy as np\n",
    "\n",
    "class TestModelInference(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # In real systems, load model artifacts and a validation sample here\n",
    "        self.model = deployed_model\n",
    "        # Use a small batch for speed\n",
    "        self.X_batch = X_test[:64]\n",
    "        self.y_batch = y_test[:64]\n",
    "\n",
    "    def test_predictions_shape(self):\n",
    "        y_pred = predict_labels(self.model, self.X_batch)\n",
    "        self.assertEqual(y_pred.shape[0], self.X_batch.shape[0], \n",
    "                         \"Prediction length should match input batch size\")\n",
    "\n",
    "    def test_inference_latency(self):\n",
    "        t0 = time.perf_counter()\n",
    "        _ = predict_labels(self.model, self.X_batch)\n",
    "        dt_ms = (time.perf_counter() - t0) * 1000.0\n",
    "        # Expect inference to be comfortably under 50 ms for this small batch\n",
    "        self.assertTrue(dt_ms < 50.0, f\"Inference too slow: {dt_ms:.2f} ms\")\n",
    "\n",
    "    def test_input_value_ranges(self):\n",
    "        # Example range checks (replace with domain-specific ranges in production)\n",
    "        # Here we assert each standardized-like feature stays within plausible bounds.\n",
    "        x_min = np.min(X_train, axis=0)\n",
    "        x_max = np.max(X_train, axis=0)\n",
    "        # Pick one sample and ensure it's within observed training bounds\n",
    "        sample = self.X_batch[0]\n",
    "        for i, val in enumerate(sample):\n",
    "            self.assertTrue(x_min[i] - 1e-6 <= val <= x_max[i] + 1e-6,\n",
    "                            f\"Feature {i} out of expected range: {val} not in [{x_min[i]}, {x_max[i]}]\")\n",
    "\n",
    "# Notebook-friendly test runner\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59314b1d",
   "metadata": {},
   "source": [
    "## Testing do's and don'ts\n",
    "\n",
    "Do:\n",
    "- Run tests on every change and before deployment\n",
    "- Add tests alongside new functionality (TDD mindset)\n",
    "- Test failure modes and edge cases (empty inputs, wrong types, out-of-range values)\n",
    "\n",
    "Don't:\n",
    "- Over-test trivial, stable library internals (e.g., re-checking sklearn math)\n",
    "- Write redundant or flaky tests that slow development\n",
    "\n",
    "Tip: Factor model I/O and preprocessing into small functions so they can be unit-tested independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12420cb",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Extract `predict_labels` into a small `src/` module and write tests in a `tests/` folder\n",
    "- Add CI to run tests automatically on push (e.g., GitHub Actions)\n",
    "- Extend tests to cover preprocessing, postprocessing, and schema validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d1ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that model prediction outputs are only {0, 1}\n",
    "import unittest\n",
    "import numpy as np\n",
    "\n",
    "class TestModelPredictionValues(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Reuse model and test split prepared earlier; provide safe fallbacks\n",
    "        try:\n",
    "            self.model = deployed_model\n",
    "        except NameError:\n",
    "            try:\n",
    "                self.model = model\n",
    "            except NameError:\n",
    "                from sklearn.linear_model import LogisticRegression\n",
    "                from sklearn.datasets import make_classification\n",
    "                X_tmp, y_tmp = make_classification(n_samples=500, n_features=8, n_informative=5, random_state=0)\n",
    "                self.model = LogisticRegression(max_iter=500, solver=\"liblinear\", random_state=0).fit(X_tmp, y_tmp)\n",
    "        try:\n",
    "            self.X_test = X_test\n",
    "        except NameError:\n",
    "            try:\n",
    "                self.X_test = X\n",
    "            except NameError:\n",
    "                from sklearn.datasets import make_classification\n",
    "                X_tmp, _ = make_classification(n_samples=200, n_features=8, n_informative=5, random_state=0)\n",
    "                self.X_test = X_tmp[:64]\n",
    "\n",
    "    def test_prediction_output_values(self):\n",
    "        print(\"Running test_prediction_output_values test case\")\n",
    "        # Use helper if defined; otherwise call model.predict directly\n",
    "        try:\n",
    "            y_pred = predict_labels(self.model, self.X_test)\n",
    "        except NameError:\n",
    "            y_pred = self.model.predict(self.X_test)\n",
    "        unique_values = np.unique(y_pred)\n",
    "        for value in unique_values:\n",
    "            self.assertIn(int(value), [0, 1], f\"Unexpected prediction value: {value}\")\n",
    "\n",
    "# Run just this test class to avoid re-running earlier suites\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestModelPredictionValues)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
