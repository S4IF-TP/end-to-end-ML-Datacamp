{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d3318e4",
   "metadata": {},
   "source": [
    "# Module 4: Feature Engineering & Selection\n",
    "\n",
    "**Course**: End-to-End Machine Learning (Datacamp)  \n",
    "**Case Study**: CardioCare Heart Disease Prediction  \n",
    "**Author**: Seif\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Feature engineering and selection are crucial steps that build upon data preparation.\n",
    "\n",
    "In this module, we cover:\n",
    "1. What is feature engineering?\n",
    "2. Normalization (scaling to 0-1)\n",
    "3. Standardization (mean=0, variance=1)\n",
    "4. What constitutes a good feature?\n",
    "5. Feature selection with sklearn\n",
    "6. Random Forest-based feature importance\n",
    "\n",
    "**Remember**: More features isn't always better â€” it's about selecting the RIGHT features!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e91ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee3dc29",
   "metadata": {},
   "source": [
    "## 1. What is Feature Engineering?\n",
    "\n",
    "**Feature engineering** is the process of creating features that enhance ML model performance.\n",
    "\n",
    "### Benefits:\n",
    "- âœ… **Simpler models** â€” easier deployment and maintenance\n",
    "- âœ… **Faster training** â€” fewer or more relevant features speed up learning\n",
    "- âœ… **Better interpretability** â€” easier to explain model decisions\n",
    "- âœ… **Improved performance** â€” capture essential aspects of data\n",
    "\n",
    "### Two Main Approaches:\n",
    "1. **Modify existing features** â€” transform, scale, or combine current features\n",
    "2. **Create new features** â€” engineer completely new variables from raw data\n",
    "\n",
    "**Key Principle**: More features â‰  better. Select features that capture essential aspects of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3956a",
   "metadata": {},
   "source": [
    "## 2. Normalization\n",
    "\n",
    "**Normalization** scales numeric features to a range of **0 to 1**.\n",
    "\n",
    "### Why Normalize?\n",
    "- Ensures no feature dominates the model due to its scale\n",
    "- Essential when features have different ranges\n",
    "- Required for scale-sensitive algorithms\n",
    "\n",
    "### When to Use:\n",
    "- **K-Nearest Neighbors (KNN)** â€” distance-based, very sensitive to scale\n",
    "- **Neural Networks** â€” gradient descent converges faster with normalized inputs\n",
    "- **Algorithms with distance metrics** â€” clustering, SVM with RBF kernel\n",
    "\n",
    "### Formula:\n",
    "$$x_{normalized} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39830cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization using sklearn\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Create a normalizer object\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Normalize the DataFrame (assuming heart_disease_df is loaded)\n",
    "# normalized_df = normalizer.fit_transform(heart_disease_df)\n",
    "\n",
    "# Returns a normalized version where all features are scaled to 0-1 range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d00ca62",
   "metadata": {},
   "source": [
    "## 3. Standardization\n",
    "\n",
    "**Standardization** scales features to have a **mean of 0** and **variance of 1**.\n",
    "\n",
    "### Why Standardize?\n",
    "- Centers data around zero\n",
    "- Makes variance comparable across features\n",
    "- Helps algorithms that assume normally distributed inputs\n",
    "\n",
    "### When to Use:\n",
    "- **Support Vector Machines (SVM)** â€” assumes features are centered\n",
    "- **Linear Regression** â€” improves numerical stability\n",
    "- **Principal Component Analysis (PCA)** â€” requires standardized data\n",
    "- **Algorithms sensitive to variance** â€” logistic regression, ridge/lasso\n",
    "\n",
    "### Formula:\n",
    "$$x_{standardized} = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "Where $\\mu$ is the mean and $\\sigma$ is the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de501af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization using sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a standard scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize the DataFrame\n",
    "# standardized_df = scaler.fit_transform(heart_disease_df)\n",
    "\n",
    "# Returns a standardized version where features have mean=0 and variance=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de83e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hands-On Exercise: Proper Standardization (Avoiding Data Leakage)\n",
    "\n",
    "âš ï¸ **Critical**: Always fit the scaler on training data only, then transform both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbae6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize 'age' on the training set and use the same standardizer to transform the 'age' column of the test set to avoid data leakage\n",
    "standardizer = StandardScaler()\n",
    "X_train['age'] = standardizer.fit_transform(X_train['age'].values.reshape(-1,1))\n",
    "X_test['age'] = standardizer.transform(X_test['age'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b441fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(X_train['age'], bins=30, alpha=0.5, label='Standardized')\n",
    "plt.legend(prop={'size': 16})\n",
    "plt.title('Histogram with Standardized Age')\n",
    "plt.xlabel('Standardized Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcca0d88",
   "metadata": {},
   "source": [
    "### Why This Code Prevents Data Leakage:\n",
    "\n",
    "1. **`fit_transform()` on training data only**  \n",
    "   - Calculates mean and std from X_train\n",
    "   - Applies standardization to X_train\n",
    "   \n",
    "2. **`transform()` on test data**  \n",
    "   - Uses the SAME mean and std from training data\n",
    "   - Does NOT recalculate statistics from X_test\n",
    "   \n",
    "3. **`.values.reshape(-1,1)`**  \n",
    "   - Converts pandas Series to numpy array\n",
    "   - Reshapes to 2D array (required by StandardScaler)\n",
    "   - `-1` means \"infer dimension\", `1` means single column\n",
    "\n",
    "### What the histogram shows:\n",
    "After standardization, age values are centered around 0 with most values between -2 and 2 (within 2 standard deviations).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d155221",
   "metadata": {},
   "source": [
    "### ðŸ“Š Normalization vs. Standardization\n",
    "\n",
    "| Aspect | Normalization | Standardization |\n",
    "|--------|---------------|-----------------|\n",
    "| **Range** | 0 to 1 | Unbounded (centered at 0) |\n",
    "| **Formula** | $(x - min) / (max - min)$ | $(x - \\mu) / \\sigma$ |\n",
    "| **Best for** | KNN, Neural Nets | SVM, Linear models |\n",
    "| **Sensitivity** | Affected by outliers | Less affected by outliers |\n",
    "| **Use case** | Bounded range needed | Normal distribution assumed |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a0c58",
   "metadata": {},
   "source": [
    "## 4. What Constitutes a Good Feature?\n",
    "\n",
    "Good features improve prediction accuracy. To select good features, we need to consider:\n",
    "\n",
    "### âœ… Relevance\n",
    "Features should be **relevant** to the prediction task.\n",
    "\n",
    "**Good**: Age, cholesterol, blood pressure â†’ directly related to heart disease  \n",
    "**Bad**: Weather on appointment day â†’ no bearing on diagnosis\n",
    "\n",
    "### âœ… Non-redundancy\n",
    "Avoid multiple features representing **similar metrics**.\n",
    "\n",
    "**Redundant**: Both \"age in years\" AND \"age in months\" â€” capture the same information  \n",
    "**Better**: Choose one (typically age in years)\n",
    "\n",
    "### âœ… Orthogonality\n",
    "Good features should be **dissimilar** (perpendicular/orthogonal) to each other.\n",
    "\n",
    "- Features that are highly correlated don't add new information\n",
    "- Orthogonal features capture different aspects of the data\n",
    "- Example: Age and cholesterol are orthogonal (independent measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a57e3",
   "metadata": {},
   "source": [
    "## 5. Feature Selection with sklearn\n",
    "\n",
    "**sklearn.feature_selection** provides a robust toolbox for selecting significant, non-redundant features.\n",
    "\n",
    "### âš ï¸ Important: Split Data First!\n",
    "Always split your data **before** feature selection to avoid **data leakage**.\n",
    "\n",
    "```python\n",
    "# Split first to prevent test data from influencing feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "Data leakage occurs when the model is exposed to test data during training or feature selection, leading to overly optimistic performance estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65378d5c",
   "metadata": {},
   "source": [
    "## 6. SelectFromModel with Random Forest\n",
    "\n",
    "**SelectFromModel** selects features based on importance weights from a fitted model.\n",
    "\n",
    "### How it Works:\n",
    "1. Train a Random Forest classifier\n",
    "2. Random Forest calculates feature importance\n",
    "3. Eliminate features considered irrelevant\n",
    "4. Keep only features that improve predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split the data first\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     heart_disease_x, heart_disease_y, test_size=0.2, random_state=42\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc5a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create and train a Random Forest classifier\n",
    "# model = RandomForestClassifier(\n",
    "#     n_jobs=-1,           # Use all available processors\n",
    "#     class_weight='balanced',  # Balance class frequencies\n",
    "#     max_depth=5,         # Limit tree depth to 5\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Fit the model on training data\n",
    "# model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Use SelectFromModel to select important features\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# selector = SelectFromModel(model, prefit=True)\n",
    "# # prefit=True means the model has already been fitted\n",
    "\n",
    "# # Get the boolean mask of selected features\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# print(\"Selected features:\", selected_features)\n",
    "# # Returns: [True, False, True, True, False, ...]\n",
    "# # True = feature is important, False = feature is not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d4279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Transform the data to keep only selected features\n",
    "# X_train_selected = selector.transform(X_train)\n",
    "# X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# # Or get the selected feature names\n",
    "# feature_names = heart_disease_x.columns\n",
    "# selected_feature_names = feature_names[selected_features]\n",
    "# print(\"Important features:\", selected_feature_names.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b0efc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hands-On Exercise: Random Forest Feature Selection\n",
    "\n",
    "Complete workflow for selecting important features using Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5384f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Define the random forest model and fit to the training data\n",
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ed478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature selection object\n",
    "model = SelectFromModel(rf, prefit=True)\n",
    "\n",
    "# Transform the training features\n",
    "X_train_transformed = model.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd9d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_features = heart_disease_df.columns[:-1]\n",
    "print(f\"Original features: {original_features}\")\n",
    "\n",
    "# Select the features deemed important by the SelectFromModel\n",
    "features_bool = model.get_support()\n",
    "\n",
    "selected_features = original_features[features_bool]\n",
    "print(f\"\\nSelected features: {selected_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256e3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    \"feature\": selected_features,\n",
    "    \"importance\": rf.feature_importances_[features_bool]\n",
    "})\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance[\"feature\"], feature_importance[\"importance\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71912d60",
   "metadata": {},
   "source": [
    "### Understanding This Exercise:\n",
    "\n",
    "1. **Train Random Forest**  \n",
    "   - Fits on training data to learn feature importance\n",
    "   - Uses balanced class weights to handle imbalance\n",
    "   \n",
    "2. **Create SelectFromModel**  \n",
    "   - `prefit=True` indicates the model is already fitted\n",
    "   - Automatically selects features above importance threshold\n",
    "   \n",
    "3. **Transform Data**  \n",
    "   - `model.transform(X_train)` removes low-importance features\n",
    "   - Returns only the columns deemed important\n",
    "   \n",
    "4. **Get Feature Names**  \n",
    "   - `model.get_support()` returns boolean array [True, False, True, ...]\n",
    "   - `original_features[features_bool]` filters to selected features only\n",
    "   \n",
    "5. **Visualize Importance**  \n",
    "   - Horizontal bar chart shows which features matter most\n",
    "   - Longer bars = higher importance for predicting heart disease\n",
    "\n",
    "### Expected Output:\n",
    "```\n",
    "Original features: Index(['age', 'sex', 'cp', 'trestbps', 'chol', ...])\n",
    "Selected features: Index(['age', 'cp', 'thalach', 'oldpeak', ...])\n",
    "```\n",
    "\n",
    "The bar chart reveals which patient health indicators are most predictive of heart disease.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db8c519",
   "metadata": {},
   "source": [
    "### Understanding Random Forest Parameters\n",
    "\n",
    "**`n_jobs=-1`**  \n",
    "Use all available CPU processors for parallel computation (speeds up training)\n",
    "\n",
    "**`class_weight='balanced'`**  \n",
    "Automatically adjusts weights inversely proportional to class frequencies  \n",
    "Helps with imbalanced datasets (e.g., more patients without disease than with)\n",
    "\n",
    "**`max_depth=5`**  \n",
    "Limits the maximum depth of each tree to 5 levels  \n",
    "Prevents overfitting by restricting tree complexity\n",
    "\n",
    "**`prefit=True`**  \n",
    "Tells `SelectFromModel` that the model has already been fitted  \n",
    "No need to fit again inside the selector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba1902",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Feature Engineering Enhances Performance**: Proper feature engineering simplifies models and improves accuracy\n",
    "2. **Normalization vs. Standardization**: Choose based on algorithm requirements\n",
    "   - Normalization (0-1) â†’ KNN, Neural Networks\n",
    "   - Standardization (mean=0) â†’ SVM, Linear models\n",
    "3. **Quality > Quantity**: More features â‰  better. Focus on relevant, non-redundant features\n",
    "4. **Avoid Data Leakage**: Always split data before feature selection\n",
    "5. **Random Forest for Selection**: Automatically identifies important features based on predictive power\n",
    "6. **Orthogonality Matters**: Features should capture different aspects of the data\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Engineering Workflow\n",
    "\n",
    "1. **Clean the data** (Module 3)\n",
    "2. **Split train/test** (avoid data leakage)\n",
    "3. **Apply scaling** (normalization or standardization)\n",
    "4. **Select features** (Random Forest, correlation analysis, domain knowledge)\n",
    "5. **Validate selection** (check performance on test set)\n",
    "6. **Iterate** (refine based on results)\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "- Datacamp: End-to-End Machine Learning Course\n",
    "- Video 4: Feature Engineering and Selection\n",
    "- [Scikit-learn Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "- [Scikit-learn Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
