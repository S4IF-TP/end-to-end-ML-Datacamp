{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7bcf6b7",
   "metadata": {},
   "source": [
    "# Module 10: Data Drift Monitoring\n",
    "\n",
    "**Course**: End-to-End Machine Learning (Datacamp)  \n",
    "**Case Study**: CardioCare Heart Disease Prediction  \n",
    "**Author**: Seif\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "- What is data drift and why it matters\n",
    "- Kolmogorov-Smirnov (KS) test for detecting drift in a single feature\n",
    "- Practical example with scipy.stats.ks_2samp\n",
    "- Remediation strategies (retrain, blend old/new data)\n",
    "- References to drift-detection libraries (Evidently, NannyML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca2fd81",
   "metadata": {},
   "source": [
    "## What is data drift?\n",
    "\n",
    "Data drift occurs when the statistical properties of input features change over time:\n",
    "- Population changes (e.g., aging demographics, improved healthcare)\n",
    "- Data collection process shifts (new devices, protocols)\n",
    "- Seasonal or environmental trends\n",
    "\n",
    "**Example**: A heart disease model trained decades ago might see fewer young patients with disease today due to better prevention.\n",
    "\n",
    "Even if model accuracy doesn't drop immediately, drift signals that the training data distribution no longer matches production, risking eventual degradation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dda5ed5",
   "metadata": {},
   "source": [
    "## Kolmogorov-Smirnov (KS) test\n",
    "\n",
    "The KS test compares two distributions (e.g., training vs. recent production) for a single feature:\n",
    "- **Test statistic**: magnitude of the maximum difference between cumulative distributions\n",
    "- **p-value**: probability of observing such a difference if distributions are identical\n",
    "\n",
    "**Rule of thumb**: p-value < 0.05 → suspect drift (distributions likely differ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c279007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: detect drift in 'cholesterol' between training and recent data\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Simulate training data (old distribution)\n",
    "np.random.seed(42)\n",
    "chol_train = np.random.normal(loc=240, scale=40, size=500)  # mean=240, sd=40\n",
    "\n",
    "# Simulate recent production data (shifted distribution)\n",
    "chol_recent = np.random.normal(loc=220, scale=35, size=300)  # mean=220, sd=35\n",
    "\n",
    "# Perform KS test\n",
    "statistic, p_value = ks_2samp(chol_train, chol_recent)\n",
    "print(f\"KS statistic: {statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"⚠️  Drift detected (p < 0.05). Distributions likely differ.\")\n",
    "else:\n",
    "    print(\"✓ No significant drift detected (p >= 0.05).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d7f5b",
   "metadata": {},
   "source": [
    "## Visualizing drift\n",
    "\n",
    "Plotting histograms or CDFs side-by-side helps interpret the KS result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66556888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms to visualize drift\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(chol_train, bins=30, alpha=0.6, label='Training (old)', color='blue')\n",
    "plt.hist(chol_recent, bins=30, alpha=0.6, label='Recent (new)', color='orange')\n",
    "plt.xlabel('Cholesterol')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram comparison')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# CDF plot\n",
    "sorted_train = np.sort(chol_train)\n",
    "sorted_recent = np.sort(chol_recent)\n",
    "plt.plot(sorted_train, np.linspace(0, 1, len(sorted_train)), label='Training CDF', color='blue')\n",
    "plt.plot(sorted_recent, np.linspace(0, 1, len(sorted_recent)), label='Recent CDF', color='orange')\n",
    "plt.xlabel('Cholesterol')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('CDF comparison (KS test measures max vertical gap)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2571881",
   "metadata": {},
   "source": [
    "## Correcting data drift\n",
    "\n",
    "Once drift is detected:\n",
    "1. **Retrain** on fresh data if you have enough recent samples.\n",
    "2. **Blend old + new** if new data is scarce: gradually increase new data proportion until sufficient.\n",
    "3. **Monitor continuously**: set up periodic KS tests (weekly/monthly) and alerts.\n",
    "4. **Automate retraining**: trigger pipelines when drift crosses a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedcec5d",
   "metadata": {},
   "source": [
    "## Beyond KS: other drift detection methods\n",
    "\n",
    "- **Population Stability Index (PSI)**: effective for categorical features or binned continuous features.\n",
    "- **Chi-squared test**: for categorical distributions.\n",
    "- **Jensen-Shannon divergence**: symmetric measure of distribution similarity.\n",
    "\n",
    "### Dedicated libraries\n",
    "\n",
    "- [**Evidently**](https://evidentlyai.com/): open-source for drift detection, model performance monitoring, and visualization.\n",
    "- [**NannyML**](https://www.nannyml.com/): post-deployment performance estimation and drift monitoring without ground truth labels.\n",
    "\n",
    "Example (Evidently snippet):\n",
    "```python\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset\n",
    "\n",
    "report = Report(metrics=[DataDriftPreset()])\n",
    "report.run(reference_data=train_df, current_data=recent_df)\n",
    "report.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d7ffd",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "- Run a KS test on another feature (e.g., `age`, `thalach`) comparing training vs. recent data.\n",
    "- Set up a scheduled job (cron/Airflow) to run drift checks weekly and log results to MLflow.\n",
    "- Integrate Evidently or NannyML into your monitoring dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb143f85",
   "metadata": {},
   "source": [
    "# Feedback loop, re-training, and labeling\n",
    "\n",
    "In this section, we connect data drift monitoring with a practical feedback loop to keep your model effective over time. You'll see how to:\n",
    "\n",
    "- Detect drift and decide when to react\n",
    "- Acquire new labels (manually, crowd-sourcing, or programmatically)\n",
    "- Retrain periodically or incrementally (online learning)\n",
    "- Avoid pitfalls of harmful feedback loops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aed99e",
   "metadata": {},
   "source": [
    "## What is a feedback loop?\n",
    "\n",
    "A feedback loop is when a system's outputs (predictions, errors, usage stats) are fed back as inputs to guide future behavior. In ML, this means we:\n",
    "\n",
    "- Observe model performance and data properties over time\n",
    "- Use those observations to decide when and how to update the model or data pipeline\n",
    "- Iterate to adapt to changing conditions, trends, or user behavior\n",
    "\n",
    "Feedback loops enable continuous learning but must be designed carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a40492",
   "metadata": {},
   "source": [
    "## Implementing a feedback loop\n",
    "\n",
    "Common strategies:\n",
    "\n",
    "1) Acquire new labels for fresh data\n",
    "- Manual annotation by domain experts\n",
    "- Crowdsourcing with quality checks\n",
    "- Programmatic/weak supervision when appropriate\n",
    "\n",
    "2) Periodic batch retraining\n",
    "- Detect drift or performance decay\n",
    "- Sample recent data, merge with historical, retrain, and validate\n",
    "- Register and deploy only if it passes acceptance thresholds\n",
    "\n",
    "3) Online/Incremental learning\n",
    "- Use algorithms that support `partial_fit` to update on small batches\n",
    "- Suitable when data arrives continuously and you need rapid adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f5517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS-based trigger + batch retraining (skeleton)\n",
    "\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Optional integrations\n",
    "try:\n",
    "    import mlflow\n",
    "except Exception:\n",
    "    mlflow = None\n",
    "\n",
    "ALPHA = 0.05  # significance level for drift detection\n",
    "\n",
    "\n",
    "def ks_drift_test(a: pd.Series, b: pd.Series, alpha: float = ALPHA) -> Tuple[float, float, bool]:\n",
    "    \"\"\"Return (statistic, p_value, drift_detected).\"\"\"\n",
    "    a = pd.Series(a).dropna().astype(float).to_numpy()\n",
    "    b = pd.Series(b).dropna().astype(float).to_numpy()\n",
    "    stat, p = ks_2samp(a, b, alternative=\"two-sided\", mode=\"auto\")\n",
    "    return stat, p, p < alpha\n",
    "\n",
    "\n",
    "# Example usage (replace with your actual feature slices)\n",
    "# january_data = df_jan[\"feature\"]\n",
    "# february_data = df_feb[\"feature\"]\n",
    "# stat, p, drift = ks_drift_test(january_data, february_data)\n",
    "# if drift:\n",
    "#     print(f\"Drift detected: D={stat:.3f}, p={p:.3g} < {ALPHA}\")\n",
    "#     # 1) fetch new labeled data\n",
    "#     # 2) retrain model (log to MLflow if available)\n",
    "#     # 3) run your validation gate and only deploy if passing thresholds\n",
    "# else:\n",
    "#     print(f\"No drift: D={stat:.3f}, p={p:.3g} ≥ {ALPHA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning example with partial_fit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Suppose you stream batches of (X_batch, y_batch)\n",
    "classes = np.array([0, 1])\n",
    "model = make_pipeline(StandardScaler(with_mean=False), SGDClassifier(loss=\"log_loss\", random_state=42))\n",
    "\n",
    "# First call to partial_fit must include 'classes'\n",
    "# model.partial_fit(X_first_batch, y_first_batch, classes=classes)\n",
    "\n",
    "# Then iteratively update as new labeled batches arrive\n",
    "# for X_batch, y_batch in stream():\n",
    "#     model.partial_fit(X_batch, y_batch)\n",
    "#     # Optionally evaluate and log to MLflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f04ab58",
   "metadata": {},
   "source": [
    "## Dangers of feedback loops\n",
    "\n",
    "Feedback loops can become harmful when model outputs influence future inputs (e.g., recommender systems reinforcing narrow content). This can cause echo chambers, bias amplification, or unsafe behaviors. Prefer human-in-the-loop controls and guardrails, especially for automated updates.\n",
    "\n",
    "For our heart disease case, feedback is more reactive: clinicians review, data is labeled carefully, and retraining is scheduled. Still, be cautious and prioritize alignment with human values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae6745",
   "metadata": {},
   "source": [
    "## Let's practice\n",
    "\n",
    "- Pick 1–3 critical features and wire the KS trigger to raise an alert when drift is detected.\n",
    "- Define a labeling pathway for a small recent slice (e.g., last 2 weeks) and retrain.\n",
    "- Add an acceptance test to your CI (see `scripts/validate_model.py`) that blocks deploys unless metrics exceed thresholds.\n",
    "- Optional: experiment with `partial_fit` on synthetic streams to see how quickly the model adapts."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
