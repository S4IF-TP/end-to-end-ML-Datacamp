{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895f9d07",
   "metadata": {},
   "source": [
    "# Module 5: Logging Experiments with MLflow\n",
    "\n",
    "**Course**: End-to-End Machine Learning (Datacamp)  \n",
    "**Case Study**: CardioCare Heart Disease Prediction  \n",
    "**Author**: Seif\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this module, you'll learn to:\n",
    "- Create and set an MLflow experiment\n",
    "- Start runs and log parameters, metrics, and models\n",
    "- Retrieve runs programmatically (get_run, search_runs)\n",
    "- Compare runs in the MLflow UI\n",
    "\n",
    "Why MLflow?\n",
    "- Keeps experiments organized and reproducible\n",
    "- Essential in clinical settings where results must be auditable\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bcc0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0edb707",
   "metadata": {},
   "source": [
    "## 1) Create an experiment\n",
    "\n",
    "Set or create an experiment with `mlflow.set_experiment` so all runs are grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee40d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"cardiocare-heart-disease\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "print(f\"Active experiment: {experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3601b4e5",
   "metadata": {},
   "source": [
    "## 2) Run an experiment and log params/metrics\n",
    "\n",
    "We'll train a simple Logistic Regression on a synthetic binary dataset (stand-in for CardioCare data), and log parameters and metrics to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad1a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a toy binary classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=10, n_informative=6, n_redundant=2,\n",
    "    random_state=42, class_sep=1.2\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and log with MLflow\n",
    "with mlflow.start_run() as run:\n",
    "    model = LogisticRegression(max_iter=1000, C=1.0, n_jobs=1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions and metrics\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_param(\"model\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"C\", model.C)\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    mlflow.log_metric(\"f1\", f1)\n",
    "\n",
    "    # Optionally log the model artifact\n",
    "    mlflow.sklearn.log_model(model, artifact_path=\"model\")\n",
    "\n",
    "    run_id = run.info.run_id\n",
    "    print(\"Run ID:\", run_id)\n",
    "    print({\"accuracy\": acc, \"f1\": f1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44640e16",
   "metadata": {},
   "source": [
    "## 3) Retrieve experiments programmatically\n",
    "\n",
    "- `mlflow.get_run(run_id)` returns metadata for a specific run\n",
    "- `mlflow.search_runs(experiment_names=[...])` returns a pandas DataFrame for all runs in experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23230ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: fetch the last run by searching and taking the first row\n",
    "runs_df = mlflow.search_runs(experiment_names=[experiment_name], order_by=[\"start_time DESC\"], max_results=5)\n",
    "print(\"Recent runs:\\n\", runs_df[[\"run_id\", \"metrics.accuracy\", \"metrics.f1\", \"params.C\"]])\n",
    "\n",
    "if not runs_df.empty:\n",
    "    some_run_id = runs_df.iloc[0][\"run_id\"]\n",
    "    run_info = mlflow.get_run(some_run_id)\n",
    "    print(\"\\nFetched run:\", some_run_id)\n",
    "    print(\"Params:\", run_info.data.params)\n",
    "    print(\"Metrics:\", run_info.data.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4edaaec",
   "metadata": {},
   "source": [
    "## 4) Compare runs in the MLflow UI\n",
    "\n",
    "Start the UI locally to filter, sort, and compare runs:\n",
    "\n",
    "```powershell\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "The UI reads from the local `mlruns/` folder by default. You can also point to a remote tracking server by setting `MLFLOW_TRACKING_URI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715bd3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression: log coefficients and intercept to MLflow\n",
    "# This cell trains a small Logistic Regression model and logs its learned parameters.\n",
    "import mlflow\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Small synthetic dataset for demonstration\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=6,\n",
    "    n_redundant=2,\n",
    "    random_state=42,\n",
    "    class_sep=1.25\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000, solver=\"liblinear\", random_state=42)\n",
    "\n",
    "# Initialize the MLflow experiment (as requested)\n",
    "mlflow.set_experiment(\"Logistic Regression Heart Disease Prediction\")\n",
    "\n",
    "# Start a run, fit the model, and log coefficients/intercept\n",
    "with mlflow.start_run():\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Log learned parameters (coefficients and intercept)\n",
    "    # Note: MLflow params are stored as strings, but floats are accepted and cast.\n",
    "    for idx, coef in enumerate(model.coef_[0]):\n",
    "        mlflow.log_param(f\"coef_{idx}\", float(coef))\n",
    "    mlflow.log_param(\"intercept\", float(model.intercept_[0]))\n",
    "\n",
    "    # Log a couple of simple metrics for context\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    mlflow.log_metric(\"f1\", f1)\n",
    "\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    print(f\"MLflow run_id: {run_id}\")\n",
    "    print({\"accuracy\": acc, \"f1\": f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcfa8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model using k-fold cross-validation and print confusion matrix\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Resolve dataset and model variables, with safe fallbacks\n",
    "try:\n",
    "    X_data = heart_disease_df_X\n",
    "    y_data = heart_disease_df_y\n",
    "except NameError:\n",
    "    # Fallback to synthetic data if heart_disease_df_* not defined\n",
    "    try:\n",
    "        X_data = X\n",
    "        y_data = y\n",
    "    except NameError:\n",
    "        from sklearn.datasets import make_classification\n",
    "        X_data, y_data = make_classification(\n",
    "            n_samples=1000,\n",
    "            n_features=10,\n",
    "            n_informative=6,\n",
    "            n_redundant=2,\n",
    "            random_state=42,\n",
    "            class_sep=1.25\n",
    "        )\n",
    "\n",
    "# Reuse existing model if present, else define a default Logistic Regression\n",
    "try:\n",
    "    model\n",
    "except NameError:\n",
    "    model = LogisticRegression(max_iter=1000, solver=\"liblinear\", random_state=42)\n",
    "\n",
    "# Evaluate model using k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "score = cross_val_score(model, X_data, y_data, scoring='balanced_accuracy', cv=kf)\n",
    "print(\"Cross-validation balanced_accuracy scores:\", score)\n",
    "print(\"Mean \", score.mean(), \"Std \", score.std())\n",
    "\n",
    "# Fit on full data to produce predictions for confusion matrix (demo purpose)\n",
    "model.fit(X_data, y_data)\n",
    "y_pred = model.predict(X_data)\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_data, y_pred)\n",
    "print(\"Confusion matrix (on full data):\\n\", cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
